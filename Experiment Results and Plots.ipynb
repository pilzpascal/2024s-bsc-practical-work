{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports and Experiment Loading",
   "id": "733b0a74a089f42d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from src.experiment_handling import load_experiment\n",
    "import config as cfg\n",
    "import glob\n",
    "\n",
    "from src.visualisation import (\n",
    "    visualise_experiment_results,\n",
    "    visualise_epochs_before_early_stopping\n",
    ")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42  # TrueType fonts\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# use colorblind friendly palette\n",
    "plt.style.use('tableau-colorblind10')"
   ],
   "id": "e75af4a6b4a45ebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We automatically load the most recent experiment.",
   "id": "1e0392b698a972b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "experiments = sorted(glob.glob('./' + cfg.EXP_SAVE_PATH_BASE + '*.yaml'))\n",
    "experiment = load_experiment(experiments[-1])   # this function just loads a yaml file\n",
    "\n",
    "print(f'Experiment {experiment['params']['exp']['exp_id']} loaded')\n",
    "\n",
    "train_size = experiment['params']['exp']['train_size']\n",
    "n_samples_to_acquire = experiment['params']['al']['n_samples_to_acquire']\n",
    "\n",
    "ACC_THRESHOLDS = [0.9, 0.95]\n",
    "\n",
    "SAVE_DIR = './plots'"
   ],
   "id": "83e2b1ba5ad19f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Number of samples needed to reach certain test accuracy (or error)",
   "id": "8f04de0899d60025"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Extracting results",
   "id": "6859da8298f458a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Results of Gal et al. 2017, taken directly from the paper. No per run data i available.\n",
    "\n",
    "gal_results = {\n",
    "    0.9:  {'predictive_entropy': 165, 'mutual_information': 145, 'variation_ratios': 120, 'mean_standard_deviation': 230, 'random': 255},\n",
    "    0.95: {'predictive_entropy': 355, 'mutual_information': 335, 'variation_ratios': 295, 'mean_standard_deviation': 695, 'random': 835}\n",
    "}"
   ],
   "id": "bc8e6b60ed36a00e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "steps_avg = {}\n",
    "avg_steps = {}\n",
    "\n",
    "for threshold in ACC_THRESHOLDS:\n",
    "    steps_avg[threshold] = {}\n",
    "    avg_steps[threshold] = {}\n",
    "\n",
    "    for acq_func, results in experiment['results']['acq'].items():\n",
    "        acc_runs = np.array(results['test_acc'])\n",
    "\n",
    "        if len(acc_runs) == 0:\n",
    "            continue\n",
    "\n",
    "        n_acq_steps = np.argmax(acc_runs > threshold, axis=1) * n_samples_to_acquire + train_size\n",
    "\n",
    "        steps_avg[threshold][acq_func] = np.sort(n_acq_steps)\n",
    "        # steps_avg[threshold][acq_func] = n_acq_steps\n",
    "\n",
    "        avg_run = acc_runs.mean(axis=0)\n",
    "        n_acq_steps = np.argmax(avg_run > threshold) * n_samples_to_acquire + train_size\n",
    "\n",
    "        avg_steps[threshold][acq_func] = n_acq_steps"
   ],
   "id": "35a3ae1289cfcce0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for threshold, results in steps_avg.items():\n",
    "    print('='*25 + f' {threshold} ' + '='*25)\n",
    "    print('function                    |    steps    | avg  |  std   || avg steps  || Gal et al.')\n",
    "    print('-'*90)\n",
    "\n",
    "    for func, vals in results.items():\n",
    "        st_avg = steps_avg[threshold][func]\n",
    "        avg_st = avg_steps[threshold][func]\n",
    "        print(f\"{func:25s}   {st_avg}   {st_avg.mean():.1f}  {st_avg.std():4.1f}   ||    {avg_st}     ||    {gal_results[threshold][func]}\")\n",
    "    print()"
   ],
   "id": "3610d75944b47a17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## `Step-Averge` vs `Average-Step`",
   "id": "33fe95b0c1ace791"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we show that when determining how many steps were needed to reach a certain accuracy (or err) value, it makes a difference whether you first determine the steps and then take the average of that, `Step-Average`, or first average the runs and then determine the steps `Average-Step`.",
   "id": "7986974f60b052c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "thresholds_met = (steps_avg[0.95]['random'] - train_size) / n_samples_to_acquire\n",
    "avg_threshold_met = (avg_steps[0.95]['random'] - train_size) / n_samples_to_acquire"
   ],
   "id": "32f959cd0c9c5abd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_acc = np.array(experiment['results']['acq']['random']['test_acc'])\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# for i, c in enumerate(['#CC6677', '#AA4499', '#882255']):\n",
    "for i, c in enumerate(['#006BA4', '#5F9ED1', '#A2C8EC']):\n",
    "# for i, c in enumerate(['#5F9ED1', '#5F9ED1', '#5F9ED1']):\n",
    "    ax.plot(test_acc[i], color=c, label=f'run {i+1}')\n",
    "    # ax.plot(test_acc[i], color=c)\n",
    "    ax.axvline(thresholds_met[i], color=c, alpha=0.7)\n",
    "\n",
    "# Average of the points where the individual runs reach the threshold\n",
    "# ax.axvline(thresholds_met.mean(), color='#5F9ED1', linewidth=3, alpha=0.7, linestyle='dashed')\n",
    "ax.axvline(thresholds_met.mean(), color='grey', linewidth=3, alpha=0.7, linestyle='dashed')\n",
    "\n",
    "# Average run\n",
    "ax.plot(np.mean(test_acc, axis=0), color='orange', linewidth=3, alpha=0.8, label='average run')\n",
    "# Point where the average run reaches the threshold\n",
    "ax.axvline(avg_threshold_met, color='orange', linewidth=3, alpha=0.8, linestyle='dotted')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel('Error rate (1-accuracy) [%]')\n",
    "ax.set_xlabel('Number of acquired samples')\n",
    "ax.set_title('Steps needed to reach certain test error - individual runs vs average')\n",
    "\n",
    "# Line at 5% error\n",
    "# ax.text(-9, 0.9463, '5%')\n",
    "ax.text(-7.3, 0.9463, '5')\n",
    "ax.axhline(0.95, linestyle='--', color='grey')\n",
    "\n",
    "# Axes ticks\n",
    "# ax.set_yticks(np.arange(.6, 1.05, 0.1), [f'{100-n}%' for n in range(60, 101, 10)])\n",
    "ax.set_yticks(np.arange(.6, 1.05, 0.1), [f'{100-n}' for n in range(60, 101, 10)])\n",
    "# ax.set_yticks(np.arange(.6, 1.05, 0.1).tolist()+[0.95], [f'{100-n}' for n in list(range(60, 101, 10))+[95]])\n",
    "ax.set_xticks(\n",
    "    np.linspace(0, len(acc_runs[0])-1, 11),\n",
    "    np.linspace(0, len(acc_runs[0])-1, 11, dtype=int) * n_samples_to_acquire + train_size\n",
    ")\n",
    "\n",
    "ax.grid(axis='y', linewidth=0.7, alpha=0.7)\n",
    "ax.legend()\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Make the layout tight\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(SAVE_DIR + '/random_steps.pdf', format='pdf', dpi=300)"
   ],
   "id": "6dabbdaa21b5b4ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Absolute comparison",
   "id": "d327dc8f179f70e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Acquisition functions\n",
    "functions = ['ME', 'BALD', 'VR', 'MSTD', 'Random']\n",
    "\n",
    "x = np.arange(len(functions))  # positions for acquisition functions\n",
    "width = 0.12  # width of each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars: 6 groups for each function (Step-Avg. 90, Avg.-Step 90, Gal 90, Step-Avg. 95, Avg.-Step 95, Gal 95)\n",
    "ax.bar(x - 2.5 * width, np.mean(list(steps_avg[0.9].values()), axis=1), width, label='Step-Avvg. 10%', color='white', edgecolor='steelblue', hatch='/')\n",
    "ax.bar(x - 1.5 * width, list(avg_steps[0.9].values()), width, label='Avg.-Step 10%', color='steelblue')\n",
    "ax.bar(x - 0.5 * width, list(gal_results[0.9].values()), width, label='Gal et al. 10%', color='skyblue')\n",
    "\n",
    "ax.bar(x + 0.5 * width, np.mean(list(steps_avg[0.95].values()), axis=1), width, label='Step-Avg. 5%', color='white', edgecolor='orange', hatch='/')\n",
    "ax.bar(x + 1.5 * width, list(avg_steps[0.95].values()), width, label='Avg.-Step 5%', color='orange')\n",
    "ax.bar(x + 2.5 * width, list(gal_results[0.95].values()), width, label='Gal et al. 5%', color='gold')\n",
    "\n",
    "# Error bars based on standard deviation\n",
    "ax.errorbar(x - 2.5 * width, np.mean(list(steps_avg[0.9].values()), axis=1), yerr=np.std(list(steps_avg[0.9].values()), axis=1), color='steelblue', linestyle='none', capsize=3)\n",
    "ax.errorbar(x + 0.5 * width, np.mean(list(steps_avg[0.95].values()), axis=1), yerr=np.std(list(steps_avg[0.95].values()), axis=1), color='orange', linestyle='none', capsize=3)\n",
    "\n",
    "# Add baseline line at 1.0\n",
    "# ax.axhline(1.0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel('Number of samples acquired')\n",
    "ax.set_xlabel('Acquisition function')\n",
    "ax.set_title('Comparison of acquisition functions (absolute)')\n",
    "ax.set_xticks(range(len(functions)))\n",
    "ax.set_xticklabels(functions)\n",
    "# ax.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "ax.grid(axis='y', linewidth=0.7, alpha=0.7)\n",
    "ax.legend()\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Make the layout tight\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save a high resolution version\n",
    "fig.savefig(SAVE_DIR + '/acquisition_function_comparison_abs.pdf', format='pdf', dpi=300)"
   ],
   "id": "15b6d7e1dc5d1cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Relative comparison",
   "id": "accbe874045187cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To compare our results with those of Gal et al. (2017), we normalize each value by dividing it by the corresponding result of the random acquisition baseline.\n",
    "\n",
    "For example, in our experiments, reaching 90% accuracy required **130** samples for `BALD` and *200* for `Random`, whereas Gal et al. reported **145** for `BALD` and *255* for `Random`.\n",
    "After normalization, this means `BALD` achieved 90% accuracy using **65%** of the samples compared to the random baseline in our experiments, and **56%** in Gal et al.â€™s results."
   ],
   "id": "4c92da169f3ee64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def relativize_results(results: dict, dict_key='random', verbose: bool = False):\n",
    "\n",
    "    baseline = np.mean(results[dict_key])\n",
    "    results_rel = {key: value/baseline for key, value in results.items() if key != dict_key}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"baseline :: {baseline:.1f}\")\n",
    "\n",
    "    return results_rel"
   ],
   "id": "267bd23e432112db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(steps_avg[0.9]['random'])\n",
    "print(avg_steps[0.9]['random'])\n",
    "print(gal_results[0.9]['random'])"
   ],
   "id": "b319198859885008"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for data, name in [(steps_avg, 'step_avg'), (avg_steps, 'avg_steps'), (gal_results, 'gal et al.')]:\n",
    "    print(name)\n",
    "    for threshold in [0.9, 0.95]:\n",
    "        vals = data[threshold]['random']\n",
    "        baseline = np.mean(vals)\n",
    "        norm_vals = vals / baseline\n",
    "        print(f\"{threshold:.2f}  :: {norm_vals.round(2)} :: {norm_vals.mean().round(2)} :: {norm_vals.std().round(3)}\")\n",
    "    print()"
   ],
   "id": "d2a391a456b7642"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "steps_avg_rel = {}\n",
    "avg_steps_rel = {}\n",
    "gal_rel = {}\n",
    "\n",
    "\n",
    "for threshold in ACC_THRESHOLDS:\n",
    "    steps_avg_rel[threshold] = relativize_results(steps_avg[threshold])\n",
    "    avg_steps_rel[threshold] = relativize_results(avg_steps[threshold])\n",
    "    gal_rel[threshold] = relativize_results(gal_results[threshold])"
   ],
   "id": "bfe4734d5c3c17ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for threshold, results in steps_avg_rel.items():\n",
    "    print('='*25 + f' {threshold} ' + '='*25)\n",
    "    print('function                    |    steps    | avg  |  std     || avg steps || Gal et al.')\n",
    "    print('-'*90)\n",
    "\n",
    "    for func, vals in results.items():\n",
    "        st_avg = steps_avg_rel[threshold][func]\n",
    "        avg_st = avg_steps_rel[threshold][func]\n",
    "        print(f\"{func:25s}   {st_avg.round(2)}   {st_avg.mean():.2f}  {st_avg.std():4.3f}   ||   {avg_st:.2f}    ||   {gal_rel[threshold][func]:.2f}\")\n",
    "    print()"
   ],
   "id": "48eaf57216e43c01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Acquisition functions\n",
    "functions = ['ME', 'BALD', 'VR', 'MSTD']\n",
    "\n",
    "x = np.arange(len(functions))  # positions for acquisition functions\n",
    "width = 0.12  # width of each bar\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot bars: 6 groups for each function (Step-Avg. 90, Avg.-Step 90, Gal 90, Step-Avg. 95, Avg.-Step 95, Gal 95)\n",
    "ax.bar(x - 2.5 * width, np.mean(list(steps_avg_rel[0.9].values()), axis=1), width, label='Step-Avg. 10%', color='white', edgecolor='steelblue', hatch='/')\n",
    "ax.bar(x - 1.5 * width, list(avg_steps_rel[0.9].values()), width, label='Avg.-Step 10%', color='steelblue')\n",
    "ax.bar(x - 0.5 * width, list(gal_rel[0.9].values()), width, label='Gal et al. 10%', color='skyblue')\n",
    "\n",
    "ax.bar(x + 0.5 * width, np.mean(list(steps_avg_rel[0.95].values()), axis=1), width, label='Step-Avg. 5%', color='white', edgecolor='orange', hatch='/')\n",
    "ax.bar(x + 1.5 * width, list(avg_steps_rel[0.95].values()), width, label='Avg.-Step 5%', color='orange')\n",
    "ax.bar(x + 2.5 * width, list(gal_rel[0.95].values()), width, label='Gal et al. 5%', color='gold')\n",
    "\n",
    "# Error bars based on standard deviation\n",
    "ax.errorbar(x - 2.5 * width, np.mean(list(steps_avg_rel[0.9].values()), axis=1), yerr=np.std(list(steps_avg_rel[0.9].values()), axis=1), color='steelblue', linestyle='none', capsize=3)\n",
    "ax.errorbar(x + 0.5 * width, np.mean(list(steps_avg_rel[0.95].values()), axis=1), yerr=np.std(list(steps_avg_rel[0.95].values()), axis=1), color='orange', linestyle='none', capsize=3)\n",
    "\n",
    "# Add baseline line at 1.0\n",
    "# ax.axhline(1.0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel('Normalized score (relative to random)')\n",
    "ax.set_xlabel('Acquisition Function')\n",
    "ax.set_title('Comparison of acquisition functions (normalized to random baseline)')\n",
    "ax.set_xticks(range(len(functions)))\n",
    "ax.set_xticklabels(functions)\n",
    "ax.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "# ax.axhline(1.0, linestyle='--', color='grey')\n",
    "ax.grid(axis='y', linewidth=0.7, alpha=0.7)\n",
    "ax.legend()\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "\n",
    "# Make the layout tight\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save a high resolution version\n",
    "fig.savefig(SAVE_DIR + '/acquisition_function_comparison_rel.pdf', format='pdf', dpi=300)"
   ],
   "id": "3104deb5da603e8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test set accuracy and mutual information per acquisition step",
   "id": "e863c9ba41c89712"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here we plot the performance in terms of test set accuracy and mutual information for each acquisition step.\n",
    "To do so, we average the accuracy and mutual information results of all three runs into one run.\n",
    "We show the **average** as well as the **standard deviation**."
   ],
   "id": "595cbb5918a31984"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This cell changes the full length names of the acquisition functions to shorter names for plotting\n",
    "from copy import deepcopy\n",
    "\n",
    "_exp_temp = deepcopy(experiment)\n",
    "# change keys to shorter names EM, BALD, VR, MSTD, RANDOM\n",
    "_exp_temp['results']['acq'] = {\n",
    "    k.replace('predictive_entropy', 'ME')\n",
    "     .replace('mutual_information', 'BALD')\n",
    "     .replace('variation_ratios', 'VR')\n",
    "     .replace('mean_standard_deviation', 'MSTD')\n",
    "     .replace('random', 'Random'):\n",
    "        v for k, v in _exp_temp['results']['acq'].items()\n",
    "}\n",
    "\n",
    "print(experiment['results']['acq'].keys())\n",
    "print(_exp_temp['results']['acq'].keys())"
   ],
   "id": "9207c4db858e8a14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualise_experiment_results(_exp_temp, ['acc'], save_path=SAVE_DIR)",
   "id": "564088c0ee96b600"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualise_experiment_results(_exp_temp, ['inf'], save_path=SAVE_DIR)",
   "id": "ece1d775b0740f0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Early stopping",
   "id": "ff3f59dee336eeec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here we show the average number of epochs before early stopping, averaged over the three runs and additionally a moving average over that. The shaded area indicates the moving average of the standard deviation between the three runs.\n",
    "\n",
    "What is interesting to me is that it seems like the four acquisition functions typically train for longer than `Random`, and that the training time increases with dataset size."
   ],
   "id": "c3dd4a3427908ad3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualise_epochs_before_early_stopping(experiment)",
   "id": "9534216b7b16d1e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MNIST example",
   "id": "b192875224f7ad7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we plot a high resolution example of thirty MNIST samples, three from each class.",
   "id": "bf09106d985f2291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot an example of each digit from the MNIST dataset\n",
    "mnist = datasets.MNIST('../data/mnist', download=True)\n",
    "fig, axes = plt.subplots(3, 10, figsize=(15, 9))\n",
    "\n",
    "# plot two of each number\n",
    "for digit in range(10):\n",
    "    indices = np.where(mnist.targets == digit)[0]\n",
    "    for i in range(3):\n",
    "        ax = axes[i, digit]\n",
    "        ax.imshow(mnist.data[indices[i]], cmap='gray')\n",
    "        ax.axis('off')\n",
    "        if i == 0:\n",
    "            ax.set_title(f'{digit}', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=-0.75)\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(SAVE_DIR + '/mnist_example.pdf', format='pdf', dpi=300, bbox_inches='tight')"
   ],
   "id": "9598f8d52ec6d7e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
